<div align="center">

# âœ¨ Synapse  
*Where Documents Come Alive* ğŸŒŒ  

[![License: MIT](https://img.shields.io/badge/License-MIT-gold.svg?style=for-the-badge&labelColor=1a1a1a)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/Frontend-Next.js-3178C6?style=for-the-badge&logo=next.js&logoColor=white&labelColor=1a1a1a)](https://nextjs.org/)
[![Python](https://img.shields.io/badge/Backend-FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white&labelColor=1a1a1a)](https://fastapi.tiangolo.com/)
[![AI](https://img.shields.io/badge/AI-RAG%20Pipeline-magenta?style=for-the-badge&logo=openai&logoColor=white&labelColor=1a1a1a)](#)

> **The Art of Intelligent Document Conversation**  
> Static PDFs â†’ Living Dialogues âœ¨

</div>

---

## ğŸŒŒ Vision

What if your documents could **speak back**?  
Synapse transforms static files into **conversational companions**.  

Instead of scrolling endlessly, you ask questions like you would to a colleague:  
- *â€œSummarize section 2 in three lines.â€*  
- *â€œCompare results from table 3 with the conclusion.â€*  
- *â€œWhat assumptions are hidden in the methodology?â€*  

Behind this, the system runs **Retrieval-Augmented Generation (RAG)**:  
1. Splits the document into small **chunks** ğŸ“„  
2. Embeds them into a **vector space** ğŸŒ (where meaning is geometry)  
3. Retrieves the **closest knowledge** for your query ğŸ”  
4. Feeds both **your question + retrieved chunks** into an **LLM** ğŸ§   

The answer is **grounded, contextual, and transparent**.  
Not AI hallucination â€” but **AI amplified by retrieval**.

---

## ğŸŒŸ Features

- ğŸ“„ **PDF Uploads** â†’ Any report, research, policy, or notes.  
- ğŸ’¬ **Natural Language Q&A** â†’ Just ask, no syntax needed.  
- ğŸ” **Source Transparency** â†’ Exact page + snippet citations.  
- âš¡ **Local & Fast** â†’ Privacy-first, no external API calls.  
- ğŸ¨ **Elegant UI** â†’ Next.js + Tailwind design, sleek & minimal.  
- ğŸ§  **Configurable AI** â†’ Swap models, tweak chunk size, tune temperature.  

---

## ğŸ§  Technology Craftsmanship

| **Layer**           | **What We Use**                                                                                                                                                              | **Why It Matters**                                                                                   |
|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|
| **Frontend** ğŸ¨      | Next.js Â· React Â· TypeScript Â· TailwindCSS                                                                                                                                   | Smooth, modern, responsive UI.                                                                      |
| **Backend** ğŸ”§       | FastAPI (Python)                                                                                                                                                             | Lightweight, async API server for handling queries + embeddings.                                     |
| **AI Core** ğŸ§        | LangChain Â· Ollama Â· Phi-3 / LLaMA models                                                                                                                                   | Orchestrates retrieval + generation. Models run locally via Ollama.                                  |
| **Vector DB** ğŸ“Š     | ChromaDB                                                                                                                                                                     | Stores embeddings of document chunks, enabling fast semantic search.                                 |
| **Embeddings** ğŸ”®    | Sentence Transformers / Instructor models                                                                                                                                   | Converts text â†’ vectors in high-dimensional space (meaningful proximity).                            |

---

## ğŸ›ï¸ Architecture

Hereâ€™s how Synapse works under the hood:  

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   PDF Upload â”‚ ğŸ“„
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Preprocessor  â”‚ âœ‚ï¸
        â”‚ (chunk + clean) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Embeddings     â”‚ ğŸ”®
        â”‚ (Vectorization) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Vector DB      â”‚ ğŸ“Š
        â”‚   (Chroma)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 Query  â”‚   Retriever     â”‚ ğŸ”
  ğŸ’¬ â”€â”€â–¶â”‚ (find relevant) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   LLM Engine    â”‚ ğŸ§ 
        â”‚ (Phi-3 / LLaMA) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Response +     â”‚ âœ¨
        â”‚  Citations      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quickstart

### **1. Prerequisites**
- Node.js â‰¥ 18  
- Python â‰¥ 3.10  
- [Ollama](https://ollama.com/) installed  
- 4GB+ RAM  

---

### **2. Clone & Setup**
```bash
git clone https://github.com/your-username/synapse.git
cd synapse
```

#### Backend
```bash
cd server
python -m venv venv
source venv/bin/activate   # (Mac/Linux)
.\venv\Scripts\activate    # (Windows)

pip install -r requirements.txt
```

#### Frontend
```bash
cd ../client
npm install
```

---

### **3. AI Model Setup**

#### Option A: Custom Local Model (Recommended)
```bash
# Download Phi-3 Mini (3.9 GB) from HuggingFace
cp phi3.Modelfile.example phi3.Modelfile
# Update file path to gguf model
ollama create phi3-local -f phi3.Modelfile
```

#### Option B: Quick Start
```bash
ollama pull phi3:mini
```

---

### **4. Run Synapse**

**Backend (Terminal 1):**
```bash
cd server
uvicorn main:app --reload --port 8000
```

**Frontend (Terminal 2):**
```bash
cd client
npm run dev
```

ğŸŒ Visit â†’ [http://localhost:3000](http://localhost:3000)  

---

## ğŸ¯ Usage Examples

Ask your documents like a friend:  
- â€œSummarize this in 5 bullet points.â€  
- â€œWhich section mentions neural networks?â€  
- â€œExplain section 4 as if Iâ€™m 12 years old.â€  
- â€œCompare methodology with conclusion.â€  

Every answer â†’ with **citations & context** ğŸ”.  

---

## ğŸ”§ Configuration

Tune your AI in `server/.env`:

```env
MODEL_NAME=phi3-local
OLLAMA_BASE_URL=http://localhost:11434
CHUNK_SIZE=1000
TEMPERATURE=0.1
```

- `CHUNK_SIZE`: how much text per vector slice  
- `TEMPERATURE`: creativity of responses (0 = factual, 1 = creative)  

---

## ğŸŒŒ Philosophy

Synapse is not just code â€” itâ€™s **a belief**.  
That documents are meant to be **alive, not static**.  
That **AI should be transparent, not black-boxed**.  
That embeddings + neural nets can feel almost *poetic* in how they connect meaning across dimensions.  

It is research, but it is also art.  
A bridge between **mathematical rigor** and **dreamy interaction** âœ¨.  

---

## ğŸ“œ License

MIT License â€” free to use, remix, and explore.  

---

<div align="center">

âœ¨ Crafted with curiosity & code by Muskan âœ¨  
*Turning documents into conversations* ğŸŒ¸  

![Stars](https://img.shields.io/github/stars/your-username/synapse?style=social)

</div>
